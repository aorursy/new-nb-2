# Libraries
import pandas as pd

import numpy as np

import os

import time

import matplotlib.pyplot as plt

import seaborn as sns





import plotly

print("Version de Plotly installée :", plotly.__version__)

import plotly.plotly as py

plotly.tools.set_credentials_file(username='DavidDubos', api_key='fK25XpzYcp8oTGoC5ISW')

import plotly.graph_objs as go

import plotly.tools as tls

import plotly.offline as py

py.init_notebook_mode(connected=True)





print(os.listdir("../input/microsoft-malware-prediction"))

ChunksizeUneLigne = pd.read_csv('..input/microsoft-malware-prediction/train.csv',chunksize=1)

print("Type de fichier : ",type(ChunksizeUneLigne))

for chunk in ChunksizeUneLigne:

    simple_chunk = chunk

    break

print("Type de fichier : ",type(simple_chunk))
simple_chunk.info()
dtypes = {

        'MachineIdentifier':                                    'category',

        'ProductName':                                          'category',

        'EngineVersion':                                        'category',

        'AppVersion':                                           'category',

        'AvSigVersion':                                         'category',

        'IsBeta':                                               'int8',

        'RtpStateBitfield':                                     'float16',

        'IsSxsPassiveMode':                                     'int8',

        'DefaultBrowsersIdentifier':                            'float32',

        'AVProductStatesIdentifier':                            'float32',

        'AVProductsInstalled':                                  'float16',

        'AVProductsEnabled':                                    'float16',

        'HasTpm':                                               'int8',

        'CountryIdentifier':                                    'int16',

        'CityIdentifier':                                       'float32',

        'OrganizationIdentifier':                               'float16',

        'GeoNameIdentifier':                                    'float16',

        'LocaleEnglishNameIdentifier':                          'int16',

        'Platform':                                             'category',

        'Processor':                                            'category',

        'OsVer':                                                'category',

        'OsBuild':                                              'int16',

        'OsSuite':                                              'int16',

        'OsPlatformSubRelease':                                 'category',

        'OsBuildLab':                                           'category',

        'SkuEdition':                                           'category',

        'IsProtected':                                          'float16',

        'AutoSampleOptIn':                                      'int8',

        'PuaMode':                                              'category',

        'SMode':                                                'float16',

        'IeVerIdentifier':                                      'float16',

        'SmartScreen':                                          'category',

        'Firewall':                                             'float16',

        'UacLuaenable':                                         'float64', 

        'Census_MDC2FormFactor':                                'category',

        'Census_DeviceFamily':                                  'category',

        'Census_OEMNameIdentifier':                             'float32', 

        'Census_OEMModelIdentifier':                            'float32',

        'Census_ProcessorCoreCount':                            'float16',

        'Census_ProcessorManufacturerIdentifier':               'float16',

        'Census_ProcessorModelIdentifier':                      'float32', 

        'Census_ProcessorClass':                                'category',

        'Census_PrimaryDiskTotalCapacity':                      'float64', 

        'Census_PrimaryDiskTypeName':                           'category',

        'Census_SystemVolumeTotalCapacity':                     'float64',

        'Census_HasOpticalDiskDrive':                           'int8',

        'Census_TotalPhysicalRAM':                              'float32',

        'Census_ChassisTypeName':                               'category',

        'Census_InternalPrimaryDiagonalDisplaySizeInInches':    'float32', 

        'Census_InternalPrimaryDisplayResolutionHorizontal':    'float32', 

        'Census_InternalPrimaryDisplayResolutionVertical':      'float32', 

        'Census_PowerPlatformRoleName':                         'category',

        'Census_InternalBatteryType':                           'category',

        'Census_InternalBatteryNumberOfCharges':                'float64', 

        'Census_OSVersion':                                     'category',

        'Census_OSArchitecture':                                'category',

        'Census_OSBranch':                                      'category',

        'Census_OSBuildNumber':                                 'int16',

        'Census_OSBuildRevision':                               'int32',

        'Census_OSEdition':                                     'category',

        'Census_OSSkuName':                                     'category',

        'Census_OSInstallTypeName':                             'category',

        'Census_OSInstallLanguageIdentifier':                   'float16',

        'Census_OSUILocaleIdentifier':                          'int16',

        'Census_OSWUAutoUpdateOptionsName':                     'category',

        'Census_IsPortableOperatingSystem':                     'int8',

        'Census_GenuineStateName':                              'category',

        'Census_ActivationChannel':                             'category',

        'Census_IsFlightingInternal':                           'float16',

        'Census_IsFlightsDisabled':                             'float16',

        'Census_FlightRing':                                    'category',

        'Census_ThresholdOptIn':                                'float16',

        'Census_FirmwareManufacturerIdentifier':                'float16',

        'Census_FirmwareVersionIdentifier':                     'float32',

        'Census_IsSecureBootEnabled':                           'int8',

        'Census_IsWIMBootEnabled':                              'float16',

        'Census_IsVirtualDevice':                               'float16',

        'Census_IsTouchEnabled':                                'int8',

        'Census_IsPenCapable':                                  'int8',

        'Census_IsAlwaysOnAlwaysConnectedCapable':              'float16',

        'Wdft_IsGamer':                                         'float16',

        'Wdft_RegionIdentifier':                                'float16',

        'HasDetections':                                        'int8'

        }
train = pd.read_csv('../input/microsoft-malware-prediction/train.csv',dtype=dtypes)

train=train[:40000] ############################ pour la certification

train.info()

test = pd.read_csv('../input/microsoft-malware-prediction/test.csv',dtype=dtypes)
train.shape

print("Nombre de lignes :" , train.shape[0])

(train.isnull().sum()/train.shape[0]*100).sort_values(ascending=False).head(15)
Colonnes_a_supprimer=[]  

Colonnes_a_supprimer.append("PuaMode")

Colonnes_a_supprimer.append("Census_ProcessorClass")
train.DefaultBrowsersIdentifier.value_counts().head(20)
#Remplace les valeurs manquantes par 0

train.DefaultBrowsersIdentifier.fillna(0, inplace=True)

test.DefaultBrowsersIdentifier.fillna(0, inplace=True)
train.Census_InternalBatteryType.describe()
train.Census_InternalBatteryType.value_counts().head(78)
trans_dict = {

    '#': 'unknown', 'unkn': 'unknown', np.nan: 'unknown'

}

train.replace({'Census_InternalBatteryType': trans_dict}, inplace=True)

test.replace({'Census_InternalBatteryType': trans_dict}, inplace=True)
train.SmartScreen.describe()
train.SmartScreen.value_counts().head(21)
#Colonne :SmartScreen

convertir = {

    'off': 'Off',  '&#x01;': '1','&#x02;': '2','&#x03;': '3', 'on': 'On', 'requireadmin': 'RequireAdmin', 'OFF': 'Off', 

    'Promt': 'Prompt', 'requireAdmin': 'RequireAdmin', 'prompt': 'Prompt', 'warn': 'Warn', 

    '00000000': '0', '&#x03;': '3',np.nan: '0'}

train.replace({'SmartScreen': convertir}, inplace=True)

test.replace({'SmartScreen': convertir}, inplace=True)
sns.set(rc={'figure.figsize':(15, 8)})

sns.countplot(x="SmartScreen", hue="HasDetections",  palette="PRGn", data=train)

plt.title("SmartScreen counts")

plt.xticks(rotation='vertical')

plt.show()
train['SmartScreen'].value_counts(dropna=False, normalize=True).cumsum()


#Remplace les valeurs manquantes par 'ExistsNotSet'

train.SmartScreen.fillna('ExistsNotSet', inplace=True)

test.SmartScreen.fillna('ExistsNotSet', inplace=True)

train.OrganizationIdentifier.value_counts().head(15)
#Remplace les valeurs manquantes par 0

train.OrganizationIdentifier.fillna(0, inplace=True)

test.OrganizationIdentifier.fillna(0, inplace=True)
asymetrie_df = pd.DataFrame([{'Colonne': c,  'Tx asymétrie': train[c].value_counts(normalize=True).values[0] * 100} for c in train.columns])

asymetrie_df = asymetrie_df.loc[ (asymetrie_df['Tx asymétrie']> 99),:]

asymetrie_df
# Exemple :

train.IsBeta.value_counts().head()
print("Il y a", asymetrie_df.shape[0], "colonnes supplémentaires qui seront supprimées")
Colonnes_a_supprimer.extend(asymetrie_df.Colonne.tolist())

Colonnes_a_supprimer
Colonnes_a_supprimer.remove('PuaMode') # on retire PuaMode car présent deux fois dans la liste des colonnes à supprimer
print("Nombre de colonnes à supprimer : ",len(Colonnes_a_supprimer))
train_corr=train.copy()

# copie du dataframe train et suppression  des colonnes
train_corr.drop(Colonnes_a_supprimer, axis=1, inplace=True)

print(train_corr.shape,train.shape)

cate_cols = train_corr.select_dtypes(include='category').columns.tolist()
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

train_corr.dropna(inplace=True)

for col in cate_cols:

    train_corr[col] = le.fit_transform(train_corr[col])
cols = train_corr.columns.tolist()

plt.figure(figsize=(15,15))

co_cols = cols[:20]

co_cols.append('HasDetections') # on rajoute la colonne détection au jeu de données

sns.heatmap(train_corr[co_cols].corr(), cmap='RdBu_r', annot=True, center=0.0)

plt.title('Correlation de la colonne 1 à 20')

plt.show()
# AvSigVersion / EngineVersion on garde le plus grand nombre de valeur unique

print(train_corr.OsVer.nunique())

print(train_corr.Platform.nunique())
#Colonne Platform à supprimer 

Colonnes_a_supprimer.append("Platform")
plt.figure(figsize=(15,15))

co_cols = cols[20:40]

co_cols.append('HasDetections') # on rajoute la colonne détection au jeu de données

sns.heatmap(train_corr[co_cols].corr(), cmap='RdBu_r', annot=True, center=0.0)

plt.title('Correlation entre la colonne 20 à 40')

plt.show()
plt.figure(figsize=(15,15))

co_cols = cols[40:70]

#co_cols.append('HasDetections') # on rajoute la colonne détection au jeu de données

sns.heatmap(train_corr[co_cols].corr(), cmap='RdBu_r', annot=True, center=0.0)

plt.title('Correlation entre la colonne 40 à 70')

plt.show()
print(train_corr.Census_OSEdition.nunique())

print(train_corr.Census_OSSkuName.nunique())
#Colonne Census_OSSkuName à supprimer 

Colonnes_a_supprimer.append("Census_OSSkuName")
print(train_corr.Census_OSInstallLanguageIdentifier.nunique())

print(train_corr.Census_OSUILocaleIdentifier.nunique())
#Colonne Census_OSInstallLanguageIdentifier à supprimer 

Colonnes_a_supprimer.append("Census_OSInstallLanguageIdentifier")
corr = train_corr.corr()

high_corr = (corr >= 0.99).astype('uint8')

plt.figure(figsize=(15,15))

sns.heatmap(high_corr, cmap='RdBu_r', annot=True, center=0.0)

plt.show()
print(train_corr.Census_OSArchitecture.nunique())

print(train_corr.Processor.nunique())
#Colonne Census_OSArchitecture ou Processorà supprimer 

Colonnes_a_supprimer.append("Census_OSArchitecture")
# L'identifiant unique MachineIdentifier n'est d'aucune utilité pour la prédiction des malewares.



Colonnes_a_supprimer.append('MachineIdentifier')
Colonnes_a_supprimer
print("Nombre total de colonnes à supprimer : ",len(Colonnes_a_supprimer))
train.drop(Colonnes_a_supprimer, axis=1, inplace=True)

train.shape
test.drop(Colonnes_a_supprimer, axis=1, inplace=True)

test.shape
total=train.shape[0]

train.drop_duplicates(inplace=True)

print("Nombre de lignes supprimées : " ,total-train.shape[0])
#Remplacement des valeurs manquantes par la valeure la plus courante

for col in train.columns.tolist(): 

    train[col] = train[col].fillna(train[col].mode()[0])
#Remplacement des valeurs manquantes par la valeure la plus courante

for col in test.columns.tolist():

    test[col] = test[col].fillna(test[col].mode()[0])
for col in train.columns.tolist():  

    if train[col].dtype.name =='category':

        print("\n Colonne :" + col)

        print (train[col].unique())

        print (train[col].value_counts())
for col in train.columns.tolist():  

    if train[col].dtype.name !='category':

        print("\n Colonne :" + col)

        print (train[col].unique())

        print (train[col].value_counts())
categorical_columns = []

numerical_columns = []

binary_columns = []

for col in train.columns:

    if train[col].dtype.name =='category':

        categorical_columns.append(col)

    if train[col].dtype.name !='category':

        if train[col].nunique() == 2:

            binary_columns.append(col)

        else:

            numerical_columns.append(col)

categories_list = []

categories_list.append(len(categorical_columns))

categories_list.append(len(numerical_columns))

categories_list.append(len(binary_columns))



categories_df = pd.DataFrame(categories_list, 

                             index=["Categorical_features", "Numerical_features", "Binary_features"])



categories_df = categories_df.transpose().plot(kind="barh", figsize=(21, 10), title="Nombre de type de features")
# source : https://www.kaggle.com/shaz13/excelsior-microsoft-malware-eda-baseline

def inspect(df):

    """Returns a inspection dataframe"""

    inspect_dataframe = pd.DataFrame({'DataType': df.dtypes, 'Unique values': df.nunique() ,

                  'Number of missing values': df.isnull().sum() ,

                  'Percentage missing': (df.isnull().sum() / len(df)) * 100,

                                      'Memory Usage (MB)':round(df.memory_usage(index=False) / 1024, 2)

                                     }).sort_values(by='Number of missing values', ascending = False)

    inspect_dataframe['Variance'] = df[inspect_dataframe.index].var()

    inspect_dataframe['Mean'] = df[inspect_dataframe.index].mean()

    inspect_dataframe['Min'] = df[inspect_dataframe.index].min()

    inspect_dataframe['Max'] = df[inspect_dataframe.index].max()

    return inspect_dataframe
detect_non = (train["HasDetections"]==0).sum()

detect_oui = (train["HasDetections"]==1).sum()



print("Malwares détectés : Non =",detect_non,", Oui =",detect_oui)
labels = '1 (Malwares détectés)','0 (Malwares non détectés)'

values = [detect_oui, detect_non]



trace = go.Pie(labels=labels, values=values)



py.iplot([trace], filename='Malwares détectés ')
# source : https://www.kaggle.com/artgor/is-this-malware-eda-fe-and-lgb-updated

# fonction graphique plotly

def plot_categorical_feature(col, only_bars=False, top_n=10, by_touch=False):

    top_n = top_n if train[col].nunique() > top_n else train[col].nunique()

    print(f"{col} a {train[col].nunique()} valeurs uniques de type : {train[col].dtype}.")

    print(train[col].value_counts(normalize=True, dropna=False).head())

    if not by_touch:

        if not only_bars:

            df = train.groupby([col]).agg({'HasDetections': ['count', 'mean']})

            df = df.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()

            data = [go.Bar(x=df.index, y=df['HasDetections']['count'].values, name='Nombre'),

                    go.Scatter(x=df.index, y=df['HasDetections']['mean'], name='taux détections', yaxis='y2')]



            layout = go.Layout(dict(title = f"Nombre de {col} par top- {top_n} catégories et valeur cible",

                                xaxis = dict(title = f'{col}',

                                             showgrid=False,

                                             zeroline=False,

                                             showline=False,),

                                yaxis = dict(title = 'Nombre',

                                             showgrid=False,

                                             zeroline=False,

                                             showline=False,),

                                yaxis2=dict(title='Taux détections', overlaying='y', side='right')),

                           legend=dict(orientation="v"))



        else:

            top_cat = list(train[col].value_counts(dropna=False).index[:top_n])

            df0 = train.loc[(train[col].isin(top_cat)) & (train['HasDetections'] == 1), col].value_counts().head(10).sort_index()

            df1 = train.loc[(train[col].isin(top_cat)) & (train['HasDetections'] == 0), col].value_counts().head(10).sort_index()

            data = [go.Bar(x=df0.index, y=df0.values, name='1 (Malwares détectés)'),

                    go.Bar(x=df1.index, y=df1.values, name='0 (Malwares non détectés)')]



            layout = go.Layout(dict(title = f"Nombre de {col} par top- {top_n}",

                                xaxis = dict(title = f'{col}',

                                             showgrid=False,

                                             zeroline=False,

                                             showline=False,),

                                yaxis = dict(title = 'Counts',

                                             showgrid=False,

                                             zeroline=False,

                                             showline=False,),

                                ),

                           legend=dict(orientation="v"), barmode='group')

        

        py.iplot(dict(data=data, layout=layout))

        

    else:

        top_n = 10

        top_cat = list(train[col].value_counts(dropna=False).index[:top_n])

        df = train.loc[train[col].isin(top_cat)]



        df1 = train.loc[train['Census_IsTouchEnabled'] == 1]

        df0 = train.loc[train['Census_IsTouchEnabled'] == 0]



        df0_ = df0.groupby([col]).agg({'HasDetections': ['count', 'mean']})

        df0_ = df0_.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()

        df1_ = df1.groupby([col]).agg({'HasDetections': ['count', 'mean']})

        df1_ = df1_.sort_values(('HasDetections', 'count'), ascending=False).head(top_n).sort_index()

        data1 = [go.Bar(x=df0_.index, y=df0_['HasDetections']['count'].values, name='Nbr appareils tactiles'),

                go.Scatter(x=df0_.index, y=df0_['HasDetections']['mean'], name='Taux de détection d appareils non tactiles', yaxis='y2')]

        data2 = [go.Bar(x=df1_.index, y=df1_['HasDetections']['count'].values, name='Nbr appareils tactiles'),

                go.Scatter(x=df1_.index, y=df1_['HasDetections']['mean'], name='Taux de détection d appareils tactiles', yaxis='y2')]



        layout = go.Layout(dict(title = f"Nombre de {col} par top- {top_n} catégories d appareils non tactiles",

                            xaxis = dict(title = f'{col}',

                                         showgrid=False,

                                         zeroline=False,

                                         showline=False,

                                         type='category'),

                            yaxis = dict(title = 'Counts',

                                         showgrid=False,

                                         zeroline=False,

                                         showline=False,),

                                    yaxis2=dict(title='taux de Detection', overlaying='y', side='right'),

                            ),

                       legend=dict(orientation="v"), barmode='group')



        py.iplot(dict(data=data1, layout=layout))

        layout['title'] = f"Nombre de {col} par top- {top_n} catégories Taux de détection d appareils tactiles"

        py.iplot(dict(data=data2, layout=layout))


plot_categorical_feature('IsProtected', True)
plot_categorical_feature('AppVersion')
plot_categorical_feature('Census_IsTouchEnabled', True)
plot_categorical_feature('Census_OSInstallTypeName', True)
plot_categorical_feature('Census_ProcessorCoreCount', True, by_touch=True)
plot_categorical_feature('Census_TotalPhysicalRAM', True, by_touch=True)
train['ram_per_processor'] = train['Census_TotalPhysicalRAM']/ train['Census_ProcessorCoreCount']
plot_categorical_feature('ram_per_processor', True, by_touch=True)
test['ram_per_processor'] = test['Census_TotalPhysicalRAM']/ test['Census_ProcessorCoreCount']
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(18,8))

train['Census_InternalPrimaryDisplayResolutionHorizontal'].value_counts().head(10).plot(kind='barh', ax=axes[0], fontsize=14).set_xlabel('Resolution Horizontal', fontsize=18)

train['Census_InternalPrimaryDisplayResolutionVertical'].value_counts().head(10).plot(kind='barh', ax=axes[1], fontsize=14).set_xlabel('Resolution Vertical', fontsize=18)

axes[0].invert_yaxis()

axes[1].invert_yaxis()
train['ResolutionRatio'] = train['Census_InternalPrimaryDisplayResolutionVertical'] / train['Census_InternalPrimaryDisplayResolutionHorizontal']

test['ResolutionRatio'] = test['Census_InternalPrimaryDisplayResolutionVertical'] / test['Census_InternalPrimaryDisplayResolutionHorizontal']
train['ResolutionRatio'].value_counts().head(10).plot(kind='barh', figsize=(14,8), fontsize=14);

plt.gca().invert_yaxis()

ratios = train['ResolutionRatio'].value_counts().head(8).index

fig, axes = plt.subplots(nrows=int(len(ratios) / 2), ncols=2, figsize=(16,14))

fig.subplots_adjust(wspace=0.2, hspace=0.4)

for i in range(len(ratios)):

    sns.countplot(x='ResolutionRatio', hue='HasDetections', data=train[train['ResolutionRatio'] == ratios[i]], ax=axes[i // 2,i % 2]);
#from plotly.offline import init_notebook_mode, iplot

sd_values = train.loc[train['Census_InternalPrimaryDisplayResolutionVertical'] < 720, 'HasDetections'].value_counts().sort_index().values

hd_values = train.loc[(train['Census_InternalPrimaryDisplayResolutionVertical'] >= 720) & (train['Census_InternalPrimaryDisplayResolutionVertical'] < 1080), 'HasDetections'].value_counts().sort_index().values

fullhd_values = train.loc[(train['Census_InternalPrimaryDisplayResolutionVertical'] >= 1080) & (train['Census_InternalPrimaryDisplayResolutionVertical'] < 2160), 'HasDetections'].value_counts().sort_index().values

k_values = train.loc[train['Census_InternalPrimaryDisplayResolutionVertical'] >= 2160, 'HasDetections'].value_counts().sort_index().values

x = ['SD', 'HD', 'FullHD', '4k']

y_0 = [sd_values[0], hd_values[0], fullhd_values[0], k_values[0]]

y_1 = [sd_values[1], hd_values[1], fullhd_values[1], k_values[1]]

trace1 = go.Bar(x=x, y=y_1, name='1 (Malwares détectés)')

trace2 = go.Bar(x=x, y=y_0, name='0 (Malwares non détectés)')

data = [trace1, trace2]

layout = go.Layout(barmode='group')

fig = go.Figure(data=data, layout=layout)

py.iplot(fig)
# OsBuildLab timestamp

from datetime import datetime



def convert(x):

    try:

        d = datetime.strptime(x.split('.')[4],'%y%m%d-%H%M')

    except:

        d = np.nan

    return d

# Erreur dans les données 171194 impossible remplacé par 171104

bad_OsBuildLab={'14314.1944.amd64fre.rs1_release.171194-2100':'14314.1944.amd64fre.rs1_release.171104-2100'}

train.replace({'OsBuildLab': bad_OsBuildLab}, inplace=True)
train['OsBuildReleaseDate'] = train['OsBuildLab'].map(convert)

test['OsBuildReleaseDate'] = test['OsBuildLab'].map(convert)

plot_categorical_feature('OsBuildReleaseDate', True)
def plot_categorical_feature_test(col, only_bars=False, top_n=10, by_touch=False):

    top_n = top_n if test[col].nunique() > top_n else test[col].nunique()

    print(f"{col} has {test[col].nunique()} unique values and type: {test[col].dtype}.")

    print(test[col].value_counts(normalize=True, dropna=False).head())

    top_cat = list(test[col].value_counts(dropna=False).index[:top_n])

    df0 = test.loc[(test[col].isin(top_cat)), col].value_counts().head(10).sort_index()

    df1 = train.loc[(train[col].isin(top_cat)), col].value_counts().head(10).sort_index()

    data = [go.Bar(x=df0.index, y=df0.values, name='Données test'),

           go.Bar(x=df1.index, y=df1.values, name='Données train')]



    layout = go.Layout(dict(title = f"Nombre de {col} par top- {top_n}",

                        xaxis = dict(title = f'{col}',

                                     showgrid=False,

                                     zeroline=False,

                                     showline=False,),

                        yaxis = dict(title = 'Counts',

                                     showgrid=False,

                                     zeroline=False,

                                     showline=False,),

                        ),

                   legend=dict(orientation="v"), barmode='group')



    py.iplot(dict(data=data, layout=layout))
plot_categorical_feature_test('OsBuildReleaseDate', True)
# AS timestamp

datedictAS = np.load('../input/timestamps/AvSigVersionTimestamps.npy')[()]

train['AvSigVersionDate'] = train['AvSigVersion'].map(datedictAS)  

test['AvSigVersionDate'] = test['AvSigVersion'].map(datedictAS)  
train['OsBuildReleaseDate']=train['OsBuildReleaseDate'].fillna(train['OsBuildReleaseDate'].mode()[0])
test['OsBuildReleaseDate']=test['OsBuildReleaseDate'].fillna(test['OsBuildReleaseDate'].mode()[0])
train['AvSigVersionDate']=train['AvSigVersionDate'].fillna(train['OsBuildReleaseDate'])
test['AvSigVersionDate']=test['AvSigVersionDate'].fillna(test['OsBuildReleaseDate'])
plot_categorical_feature('AvSigVersionDate', True)
plot_categorical_feature_test('AvSigVersionDate', True)
train=train.sort_values('AvSigVersionDate',ascending=0)
#train=train[:40000]

#train=train[:400000]

#train=train[:4000000]

train.shape
#Libraries de Machine Learning

import gc

from sklearn.preprocessing import LabelEncoder

from sklearn.preprocessing import OneHotEncoder

train_shape = train.shape

test_shape = test.shape



train_and_test = pd.concat([train,test], axis="rows", sort=False)



# libération de la mémoire en supprimant les données suivante :

del train

del test
train_and_test.info()
train_and_test['OsBuildReleaseDate'] =train_and_test['OsBuildReleaseDate'].astype('str')

train_and_test['OsBuildReleaseDate'] =train_and_test['OsBuildReleaseDate'].apply(lambda x: x.replace('-',''))

train_and_test['OsBuildReleaseDate'] =train_and_test['OsBuildReleaseDate'].apply(lambda x: x.split(' ')[0])
train_and_test['AvSigVersionDate'] =train_and_test['AvSigVersionDate'].astype('str')

train_and_test['AvSigVersionDate'] =train_and_test['AvSigVersionDate'].apply(lambda x: x.replace('-',''))

train_and_test['AvSigVersionDate'] =train_and_test['AvSigVersionDate'].apply(lambda x: x.split(' ')[0])
train_and_test.info()
categorical_columns=[]

#categorical_columns = list(train_and_test.loc[:, train_and_test.dtypes =="object"].columns)

for col in train_and_test.columns:

    if train_and_test[col].dtype.name =='category'or train_and_test[col].dtype.name =='object':

        categorical_columns.append(col)

        

def MultiLabelEncoder(columnlist,dataframe):

    for i in columnlist:

        labelencoder_X=LabelEncoder()

        dataframe[i]=labelencoder_X.fit_transform(dataframe[i])



MultiLabelEncoder(categorical_columns, train_and_test)

gc.collect()
train_and_test.info()
train = train_and_test[0:train_shape[0]]
test = train_and_test[(train_shape[0]):(train_and_test.shape[0]+1)]

# Suppression de la colonne HasDetections du jeu de test,(qui a été crée lors de la concaténation des 2 DataFrames).

test = test.drop(["HasDetections"], axis = 1)
del train_and_test

gc.collect()
# ! pip install lofo-importance
# ! pip install lightgbm
from sklearn.model_selection import KFold

from lofo import LOFOImportance, plot_importance




target = "HasDetections"

features = [col for col in train.columns if col != target]

cv = KFold(n_splits=4, shuffle=False, random_state=0)



lofo_imp = LOFOImportance(train, features, target, cv=cv, scoring="roc_auc")



importance_df = lofo_imp.get_importance()

importance_df.head()
# plot the means and standard deviations of the importances

plot_importance(importance_df, figsize=(12, 20))
from sklearn.linear_model import LogisticRegression

from sklearn.neural_network import MLPClassifier

from sklearn.tree import DecisionTreeClassifier

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier

from sklearn.metrics import classification_report, confusion_matrix

from sklearn.svm import SVC

from sklearn.neighbors import KNeighborsClassifier

import xgboost as xgb



from sklearn.model_selection import train_test_split
X = train.drop(columns='HasDetections')

y = train['HasDetections']

# create a 70/30 split of the data 

X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=42)
models = [

    ('RandomForest', RandomForestClassifier(random_state=42, n_estimators=120, max_depth=7)),

    ('DecisionTree', DecisionTreeClassifier(random_state=42, max_depth=7)),

    ('GradientBoosting', GradientBoostingClassifier(random_state=42, max_depth=6)),

    ('LogisticRegression', LogisticRegression(random_state=42, solver='liblinear')),

    ('MLPClassifier', MLPClassifier(random_state=42, solver='lbfgs', max_iter=1000)),

    #("svm", SVC()),

    ("knn" , KNeighborsClassifier(n_neighbors=5)),

    ('XgBoost',xgb.XGBClassifier(max_depth=11,objective= 'binary:logistic',seed=42))

]
train.info()
for name, model in models:

    print(name)

    model.fit(X_train, y_train)

    print("Score %.6f" % model.score(X_test, y_test))

    y_pred = model.predict(X_test)

    print(classification_report(y_test, y_pred))

    

    y_test_no = y_test.value_counts()[0]

    y_test_yes = y_test.value_counts()[1]    

    cfn_matrix = confusion_matrix(y_test, y_pred)

    cfn_norm_matrix = np.array([[1.0 / y_test_no, 1.0 / y_test_no], [1.0 / y_test_yes, 1.0 / y_test_yes]])

    norm_cfn_matrix = pd.DataFrame(cfn_matrix * cfn_norm_matrix)



    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

    sns.heatmap(cfn_matrix,

                cmap='BrBG',

                linewidths=0.5,

                annot=True,

                fmt="d",

               ax = ax[0])

    sns.heatmap(norm_cfn_matrix,

                cmap='BrBG',

                linewidths=0.5,

                annot=True,

               ax = ax[1])

    ax[0].set_xlabel("Prediction")

    ax[0].set_ylabel("Valeur Vrai")

    ax[1].set_xlabel("Prediction")

    ax[1].set_ylabel("Valeur Vrai")

    plt.show()
from sklearn.model_selection import train_test_split

from sklearn.metrics import accuracy_score

from sklearn.metrics import precision_score

from sklearn.metrics import recall_score

from sklearn.metrics import f1_score

from sklearn.metrics import classification_report

from sklearn.metrics import roc_auc_score

import time



import xgboost as xgb



import scikitplot as skplt
# create a 70/30 split of the data 

xtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=42, test_size=0.3)



from sklearn.model_selection import GridSearchCV   #Perforing grid search
# max_depth

# min_child_weight



gc.collect()



param_test1 = {

 'max_depth':[3, 5, 7, 9, 11],

 'min_child_weight':[1, 3, 5, 7, 9]

}

gsearch1 = GridSearchCV(estimator = xgb.XGBClassifier( learning_rate=0.1, n_estimators=50, gamma=0, subsample=0.9, colsample_bytree=0.6,

                                                  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1, reg_alpha = 0, 

                                                reg_lambda =1, seed=42), 

                        param_grid = param_test1, scoring='roc_auc', n_jobs=1, iid=False, cv=3, verbose = 1)



gsearch1.fit(xtrain, ytrain)

gsearch1.best_params_, gsearch1.best_score_
best_params_1 = gsearch1.best_params_

print(best_params_1)

del gsearch1

gc.collect()
# gamma

gc.collect()



param_test2 = {

 'gamma':[0, 0.2, 0.4]

}

gsearch2 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate =0.1, n_estimators=50, 

                                                      min_child_weight = best_params_1["min_child_weight"],

                                                      max_depth = best_params_1["max_depth"], subsample=0.9, colsample_bytree=0.6,

                                                  objective= 'binary:logistic', nthread=-1, scale_pos_weight=1, reg_alpha = 0, 

                                                      reg_lambda =1, seed=42), 

                        param_grid = param_test2, scoring='roc_auc', n_jobs=1, iid=False, cv=3, verbose = 1)



gsearch2.fit(xtrain, ytrain)

gsearch2.best_params_, gsearch2.best_score_
best_params_2 = gsearch2.best_params_

print(best_params_2)

del gsearch2

gc.collect()
#subsample

#colsample_bytree

gc.collect()



param_test3 = {

 'subsample':[0.4, 0.6, 0.8, 1],

 "colsample_bytree": [0.2, 0.4, 0.6, 0.8]

}

gsearch3 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate =0.1, n_estimators=50, gamma = best_params_2["gamma"],

                                                      min_child_weight = best_params_1["min_child_weight"],

                                                      max_depth = best_params_1["max_depth"],

                                                      objective= 'binary:logistic', nthread=-1, scale_pos_weight=1, reg_alpha = 0,

                                                      reg_lambda =1, seed=42), 

                        param_grid = param_test3, scoring='roc_auc', n_jobs=1, iid=False, cv=3, verbose = 1)



gsearch3.fit(xtrain, ytrain)

gsearch3.best_params_, gsearch3.best_score_
best_params_3 = gsearch3.best_params_

print(best_params_3)

del gsearch3

gc.collect()
#reg_alpha



gc.collect()



param_test4 = {

 'reg_alpha':[0, 0.3, 0.6]  

}

gsearch4 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate =0.1, n_estimators=50, gamma = best_params_2["gamma"],

                                                      min_child_weight = best_params_1["min_child_weight"],

                                                      max_depth = best_params_1["max_depth"], subsample=best_params_3["subsample"],

                                                      colsample_bytree=best_params_3["colsample_bytree"],

                                                      objective= 'binary:logistic', nthread=-1, scale_pos_weight=1,

                                                      reg_lambda =1, seed=42), 

                        param_grid = param_test4, scoring='roc_auc', n_jobs=1, iid=False, cv=3, verbose = 1)



gsearch4.fit(xtrain, ytrain)

gsearch4.best_params_, gsearch4.best_score_
best_params_4 = gsearch4.best_params_

print(best_params_4)

del gsearch4

gc.collect()
#reg_lambda





param_test5 = {

 'reg_lambda':[1, 3, 5, 7]

}

gsearch5 = GridSearchCV(estimator = xgb.XGBClassifier(learning_rate =0.1, n_estimators=50, gamma = best_params_2["gamma"],

                                                      min_child_weight = best_params_1["min_child_weight"],

                                                      max_depth = best_params_1["max_depth"], subsample=best_params_3["subsample"],

                                                      colsample_bytree=best_params_3["colsample_bytree"],

                                                      objective= 'binary:logistic', nthread=-1, scale_pos_weight=1, 

                                                      reg_alpha = best_params_4["reg_alpha"], seed=42), 

                        param_grid = param_test5, scoring='roc_auc', n_jobs=1, iid=False, cv=3, verbose = 1)



gsearch5.fit(xtrain, ytrain)

gsearch5.best_params_, gsearch5.best_score_
best_params_5 = gsearch5.best_params_

print(best_params_5)

del gsearch5

gc.collect()
start_time = time.time()



clf_xgb = xgb.XGBClassifier(learning_rate =0.03, 

                            n_estimators=4000, 

                            max_depth=best_params_1["max_depth"],

                            min_child_weight=best_params_1["min_child_weight"],

                            gamma=best_params_2["gamma"],

                            subsample=best_params_3["subsample"],

                            colsample_bytree=best_params_3["colsample_bytree"],

                            reg_alpha=best_params_4['reg_alpha'],

                            reg_lambda = best_params_5['reg_lambda'],

                            objective= 'binary:logistic',

                            nthread=-1,

                            scale_pos_weight=1,

                            seed=42)



clf_xgb.fit(xtrain, ytrain, eval_set=[(xtrain, ytrain), (xvalid, yvalid)], 

            early_stopping_rounds=100, eval_metric='auc', verbose=100)



predictions = clf_xgb.predict(xvalid)



print()

print(classification_report(yvalid, predictions))



print()

print("accuracy_score", accuracy_score(yvalid, predictions))



print()

predictions_probas = clf_xgb.predict_proba(xvalid)

print("roc-auc score for the class 1, from target 'HasDetections' ", roc_auc_score(yvalid, predictions_probas[:,1]))



print()

print("elapsed time in seconds: ", time.time() - start_time)



print()

gc.collect()
y_test_no = yvalid.value_counts()[0]

y_test_yes = yvalid.value_counts()[1]   

cfn_matrix = confusion_matrix(yvalid, predictions)

cfn_norm_matrix = np.array([[1.0 / y_test_no, 1.0 / y_test_no], [1.0 / y_test_yes, 1.0 / y_test_yes]])

norm_cfn_matrix = pd.DataFrame(cfn_matrix * cfn_norm_matrix)



fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 5))

sns.heatmap(cfn_matrix,

                cmap='BrBG',

                linewidths=0.5,

                annot=True,

                fmt="d",

               ax = ax[0])

sns.heatmap(norm_cfn_matrix,

                cmap='BrBG',

                linewidths=0.5,

                annot=True,

               ax = ax[1])

ax[0].set_xlabel("Prediction")

ax[0].set_ylabel("Valeur Vrai")

ax[1].set_xlabel("Prediction")

ax[1].set_ylabel("Valeur Vrai")

plt.show()
sns.set(rc={'figure.figsize':(8,8)})

skplt.metrics.plot_precision_recall(yvalid, predictions_probas)
sns.set(rc={'figure.figsize':(8,8)})

skplt.metrics.plot_roc(yvalid, predictions_probas)
sns.set(rc={'figure.figsize':(8,8)})

skplt.metrics.plot_ks_statistic(yvalid, predictions_probas)
sns.set(rc={'figure.figsize':(12, 18)})

xgb.plot_importance(clf_xgb, title='Feature importance', xlabel='F score', ylabel='Features')

xgb.plot_tree(clf_xgb, num_trees=2)
xgb.to_graphviz(clf_xgb, num_trees=2)
predictions_proba_test_list = []



chunck = 400000

test_times = test.shape[0] // chunck

test_rest = test.shape[0] % chunck



for i in  np.arange(0,(chunck * (test_times+1)), chunck):

    predictions_proba_test = list(clf_xgb.predict_proba(test[i:(i+chunck)])[:,1])

    predictions_proba_test_list.append(predictions_proba_test)





# flatten the list of lists

predictions_proba_test_list = [y for x in predictions_proba_test_list for y in x]



print(np.shape(predictions_proba_test_list))

print(test.shape)

gc.collect()