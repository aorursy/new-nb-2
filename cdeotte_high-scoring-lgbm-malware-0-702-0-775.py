# monkeypatches.py



# Solving memory leak problem in pandas

# https://github.com/pandas-dev/pandas/issues/2659#issuecomment-12021083

import pandas as pd, sys

from ctypes import cdll, CDLL

try:

    cdll.LoadLibrary("libc.so.6")

    libc = CDLL("libc.so.6")

    libc.malloc_trim(0)

except (OSError, AttributeError):

    libc = None



__old_del = getattr(pd.DataFrame, '__del__', None)



def __new_del(self):

    if __old_del:

        __old_del(self)

    libc.malloc_trim(0)



if libc:

    print('Applying monkeypatch for pd.DataFrame.__del__', file=sys.stderr)

    pd.DataFrame.__del__ = __new_del

else:

    print('Skipping monkeypatch for pd.DataFrame.__del__: libc or malloc_trim() not found', file=sys.stderr)
# SET THIS VARIABLE TO TRUE TO RUN KERNEL QUICKLY AND FIND BUGS

# ONLY 10000 ROWS OF DATA IS LOADED

Debug = False



import numpy as np, pandas as pd, gc, random

import matplotlib.pyplot as plt



def load(x):

    ignore = ['MachineIdentifier']

    if x in ignore: return False

    else: return True



# LOAD TRAIN AND TEST

if Debug:

    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv',dtype='category',usecols=load,nrows=10000)

else:

    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv',dtype='category',usecols=load)

df_train['HasDetections'] = df_train['HasDetections'].astype('int8')

if 5244810 in df_train.index:

    df_train.loc[5244810,'AvSigVersion'] = '1.273.1144.0'

    df_train['AvSigVersion'].cat.remove_categories('1.2&#x17;3.1144.0',inplace=True)



if Debug:

    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv',dtype='category',usecols=load,nrows=10000)

else:

    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv',dtype='category',usecols=load)

    

print('Loaded',len(df_train),'rows of TRAIN and',len(df_test),'rows of TEST')
# FREQUENCY ENCODE SEPARATELY

def encode_FE(df,col):

    vc = df[col].value_counts(dropna=False, normalize=True).to_dict()

    nm = col+'_FE'

    df[nm] = df[col].map(vc)

    df[nm] = df[nm].astype('float32')

    return [nm]



# FREQUENCY ENCODE TOGETHER

def encode_FE2(df1, df2, col):

    df = pd.concat([df1[col],df2[col]])

    vc = df.value_counts(dropna=False, normalize=True).to_dict()

    nm = col+'_FE2'

    df1[nm] = df1[col].map(vc)

    df1[nm] = df1[nm].astype('float32')

    df2[nm] = df2[col].map(vc)

    df2[nm] = df2[nm].astype('float32')

    return [nm]



# FACTORIZE

def factor_data(df_train, df_test, col):

    df_comb = pd.concat([df_train[col],df_test[col]],axis=0)

    df_comb,_ = df_comb.factorize(sort=True)

    # MAKE SMALLEST LABEL 1, RESERVE 0

    df_comb += 1

    # MAKE NAN LARGEST LABEL (need to remove attype('str') above)

    df_comb = np.where(df_comb==0, df_comb.max()+1, df_comb)

    df_train[col] = df_comb[:len(df_train)]

    df_test[col] = df_comb[len(df_train):]

    del df_comb

    

# OPTIMIZE MEMORY

def reduce_memory(df,col):

    mx = df[col].max()

    if mx<256:

            df[col] = df[col].astype('uint8')

    elif mx<65536:

        df[col] = df[col].astype('uint16')

    else:

        df[col] = df[col].astype('uint32')

        

# REDUCE CATEGORY CARDINALITY

def relax_data(df_train, df_test, col):

    cv1 = pd.DataFrame(df_train[col].value_counts().reset_index().rename({col:'train'},axis=1))

    cv2 = pd.DataFrame(df_test[col].value_counts().reset_index().rename({col:'test'},axis=1))

    cv3 = pd.merge(cv1,cv2,on='index',how='outer')

    factor = len(df_test)/len(df_train)

    cv3['train'].fillna(0,inplace=True)

    cv3['test'].fillna(0,inplace=True)

    cv3['remove'] = False

    cv3['remove'] = cv3['remove'] | (cv3['train'] < len(df_train)/10000)

    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] < cv3['test']/3)

    cv3['remove'] = cv3['remove'] | (factor*cv3['train'] > 3*cv3['test'])

    cv3['new'] = cv3.apply(lambda x: x['index'] if x['remove']==False else 0,axis=1)

    cv3['new'],_ = cv3['new'].factorize(sort=True)

    cv3.set_index('index',inplace=True)

    cc = cv3['new'].to_dict()

    df_train[col] = df_train[col].map(cc)

    reduce_memory(df_train,col)

    df_test[col] = df_test[col].map(cc)

    reduce_memory(df_test,col)

    

# DISPLAY MEMORY STATISTICS

def display_memory(df_train, df_test):

    print(len(df_train),'rows of training data use',df_train.memory_usage(deep=True).sum()//1e6,'Mb memory!')

    print(len(df_test),'rows of test data use',df_test.memory_usage(deep=True).sum()//1e6,'Mb memory!')



# CONVERT TO CATEGORIES

def categorize(df_train, df_test, cols):

    for col in cols:

        df_train[col] = df_train[col].astype('category')

        df_test[col] = df_test[col].astype('category')
from datetime import datetime, date, timedelta



# AS timestamp

datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]

df_train['DateAS'] = df_train['AvSigVersion'].map(datedictAS)

df_test['DateAS'] = df_test['AvSigVersion'].map(datedictAS)



# OS timestamp

datedictOS = np.load('../input/malware-timestamps-2/OSVersionTimestamps.npy')[()]

df_train['DateOS'] = df_train['Census_OSVersion'].map(datedictOS)

df_test['DateOS'] = df_test['Census_OSVersion'].map(datedictOS)



# ENGINEERED FEATURE #1

df_train['AppVersion2'] = df_train['AppVersion'].map(lambda x: np.int(x.split('.')[1]))

df_test['AppVersion2'] = df_test['AppVersion'].map(lambda x: np.int(x.split('.')[1]))



# ENGINEERED FEATURE #2

df_train['Lag1'] = df_train['DateAS'] - df_train['DateOS']

df_train['Lag1'] = df_train['Lag1'].map(lambda x: x.days//7)

df_test['Lag1'] = df_test['DateAS'] - df_test['DateOS']

df_test['Lag1'] = df_test['Lag1'].map(lambda x: x.days//7)



# ENGINEERED FEATURE #3

df_train['Lag5'] = datetime(2018,7,26) - df_train['DateAS']

df_train['Lag5'] = df_train['Lag5'].map(lambda x: x.days//1)

df_train.loc[ df_train['Lag5']<0, 'Lag5' ] = 0

df_test['Lag5'] = datetime(2018,9,27) - df_test['DateAS'] #PUBLIC TEST

df_test['Lag5'] = df_test['Lag5'].map(lambda x: x.days//1)

df_test.loc[ df_test['Lag5']<0, 'Lag5' ] = 0

df_train['Lag5'] = df_train['Lag5'].astype('float32') # allow for NAN

df_test['Lag5'] = df_test['Lag5'].astype('float32') # allow for NAN



# ENGINEERED FEATURE #4

df_train['driveA'] = df_train['Census_SystemVolumeTotalCapacity'].astype('float')/df_train['Census_PrimaryDiskTotalCapacity'].astype('float')

df_test['driveA'] = df_test['Census_SystemVolumeTotalCapacity'].astype('float')/df_test['Census_PrimaryDiskTotalCapacity'].astype('float')

df_train['driveA'] = df_train['driveA'].astype('float32') 

df_test['driveA'] = df_test['driveA'].astype('float32') 



# ENGINNERED FEATURE #5

df_train['driveB'] = df_train['Census_PrimaryDiskTotalCapacity'].astype('float') - df_train['Census_SystemVolumeTotalCapacity'].astype('float')

df_test['driveB'] = df_test['Census_PrimaryDiskTotalCapacity'].astype('float') - df_test['Census_SystemVolumeTotalCapacity'].astype('float')

df_train['driveB'] = df_train['driveB'].astype('float32') 

df_test['driveB'] = df_test['driveB'].astype('float32') 



cols6=['Lag1']

cols8=['Lag5','driveB','driveA']



del df_train['DateAS'], df_train['DateOS'] #, df_train['DateBL']

del df_test['DateAS'], df_test['DateOS'] #, df_test['DateBL']

del datedictAS, datedictOS

x=gc.collect()
cols3 = []

# ENGINEERED FEATURES #6, #7, #8, #9, #10

FE = ['Census_OSVersion', 'Census_OSBuildRevision', 'Census_InternalBatteryNumberOfCharges', 'AvSigVersion', 'Lag1']

for col in FE:

    cols3 += encode_FE(df_train, col)

    encode_FE(df_test, col)

    

# ENGINEERED FEATURES #11, #12

FE2 = ['CountryIdentifier', 'Census_InternalBatteryNumberOfCharges']

for col in FE2:

    cols3 += encode_FE2(df_train, df_test, col)
CE = ['CountryIdentifier', 'SkuEdition', 'Firewall', 'Census_ProcessorCoreCount', 'Census_OSUILocaleIdentifier', 'Census_FlightRing']
cols = [x for x in df_train.columns if x not in ['HasDetections']+CE+cols3+cols6+cols8]

cols2 = CE; ct = 1

    

for col in cols.copy():

    rate = df_train[col].value_counts(normalize=True, dropna=False).values[0]

    if rate > 0.98:

        del df_train[col]

        del df_test[col]

        cols.remove(col)

        ct += 1



rmv3=['Census_OSSkuName', 'OsVer', 'Census_OSArchitecture', 'Census_OSInstallLanguageIdentifier']

rmv4=['SMode']

for col in rmv3+rmv4:

    del df_train[col]

    del df_test[col]

    cols.remove(col)

    ct +=1

    

print('Removed',ct,'variables')

x=gc.collect()
print('Factorizing...')

for col in cols+cols2+cols6:

    factor_data(df_train, df_test, col)

print('Relaxing data...')

for col in cols+cols2: relax_data(df_train, df_test, col)

print('Optimizing memory...')

for col in cols+cols2+cols6:

    reduce_memory(df_train, col)

    reduce_memory(df_test, col)

# Converting 6 variables to categorical

categorize(df_train, df_test, cols2)

    

print('Number of variables is',len(cols+cols2+cols3+cols6+cols8))

display_memory(df_train, df_test)
import lightgbm as lgb

from sklearn.model_selection import StratifiedKFold



pred_val = np.zeros(len(df_test))

folds = StratifiedKFold(n_splits=5, shuffle=True)



ct = 0

for idxT, idxV in folds.split(df_train[cols+cols2+cols3+cols6], df_train['HasDetections']):

    # TRAIN LGBM

    ct += 1; print('####### FOLD ',ct,'#########')

    df_trainA = df_train.loc[idxT]

    df_trainB = df_train.loc[idxV]

    model = lgb.LGBMClassifier(n_estimators=10000, colsample_bytree=0.5, objective='binary', num_leaves=2048,

            max_depth=-1, learning_rate=0.04)

    h=model.fit(df_trainA[cols+cols2+cols3+cols6+cols8], df_trainA['HasDetections'], eval_metric='auc',

            eval_set=[(df_trainB[cols+cols2+cols3+cols6+cols8], df_trainB['HasDetections'])], verbose=200,

            early_stopping_rounds=100)

    

    # PREDICT TEST

    del df_trainA, df_trainB; x=gc.collect()

    idx = 0; ct2 = 1; chunk = 1000000

    print('Predicting test...')

    while idx < len(df_test):

        idx2 = min(idx + chunk, len(df_test) )

        idx = range(idx, idx2)

        pred_val[idx] += model.predict_proba(df_test.iloc[idx][cols+cols2+cols3+cols6+cols8])[:,1]

        #print('Finished predicting part',ct2)

        ct2 += 1; idx = idx2
del df_train; x=gc.collect()

df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv',

            usecols=['MachineIdentifier','AvSigVersion'], nrows=len(pred_val))



# CORRECT PREDICTIONS FOR OUTLIERS IN PRIVATE TEST DATASET

from datetime import datetime

datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]

df_test['Date'] = df_test['AvSigVersion'].map(datedictAS)

df_test['HasDetections'] = pred_val / 5.0

df_test['X'] = df_test['Date'] - datetime(2018,11,20,4,0) 

df_test['X'] = df_test['X'].map(lambda x: x.total_seconds()/86400)

df_test['X'].fillna(0,inplace=True)

s = 5.813888

df_test['F'] = 1.0

df_test['F'] = 1 - df_test['X']/s

df_test.loc[df_test['X']<=0,'F'] = 1.0

df_test.loc[df_test['X']>s,'F'] = 0

df_test['HasDetections'] *= df_test['F']
# WRITE SUBMISSION FILE

df_test[['MachineIdentifier','HasDetections']].to_csv('submission.csv', index=False)
import matplotlib.pyplot as plt    

b = plt.hist(df_test['HasDetections'], bins=200)
import calendar, math



def dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)

                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1,z=0,dots=False):

    # check for timestamps

    if 'Date' not in data:

        print('Error dynamicPlot: DataFrame needs column Date of datetimes')

        return

    

    # remove detection line if category density is too small

    cv = data[(data['Date']>start) & (data['Date']<end)][col].value_counts(dropna=False)

    cvd = cv.to_dict()

    nm = cv.index.values

    th = show * len(data)

    sum = 0; lnn2 = 0

    for x in nm:

        lnn2 += 1

        sum += cvd[x]

        if sum>th:

            break

    top = min(top,len(nm))

    top2 = min(top2,len(nm),lnn2,top)



    # calculate rate within each time interval

    diff = (end-start).days*24*3600 + (end-start).seconds

    size = diff//(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5

    data_counts = np.zeros([size,2*top+1],dtype=float)

    idx=0; idx2 = {}

    for i in range(top):

        idx2[nm[i]] = i+1

    low = start

    high = add_time(start,inc_mn,inc_dy,inc_hr)

    data_times = [low+(high-low)/2]

    while low<end:

        slice = data[ (data['Date']<high) & (data['Date']>=low) ]

        #data_counts[idx,0] = len(slice)

        data_counts[idx,0] = 5000*len(slice['AvSigVersion'].unique())

        for key in idx2:

            if nan_check(key): slice2 = slice[slice[col].isna()]

            else: slice2 = slice[slice[col]==key]

            data_counts[idx,idx2[key]] = len(slice2)

            if target in data:

                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()

        low = high

        high = add_time(high,inc_mn,inc_dy,inc_hr)

        data_times.append(low+(high-low)/2)

        idx += 1



    # plot lines

    fig = plt.figure(1,figsize=(15,3))

    cl = ['r','g','b','y','m']

    ax3 = fig.add_subplot(1,1,1)

    lines = []; labels = []

    if z==1: ax3.plot(data_times,data_counts[0:idx+1,0],'k')

    for i in range(top):

        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])

        if dots: ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5]+'o')

        lines.append(tmp)

        labels.append(str(nm[i]))

    ax3.spines['left'].set_color('red')

    ax3.yaxis.label.set_color('red')

    ax3.tick_params(axis='y', colors='red')

    if col!='ones': ax3.set_ylabel('Category Density', color='r')

    else: ax3.set_ylabel('Data Density', color='r')

    #ax3.set_yticklabels([])

    if target in data:

        ax4 = ax3.twinx()

        for i in range(top2):

            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+":")

            if dots: ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+"o")

        ax4.spines['left'].set_color('red')

        ax4.set_ylabel('Detection Rate', color='k')

    if title!='': plt.title(title)

    if legend==1: plt.legend(lines,labels,loc=2)

    plt.show()

        

# INCREMENT A DATETIME

def add_time(sdate,months=0,days=0,hours=0):

    month = sdate.month -1 + months

    year = sdate.year + month // 12

    month = month % 12 + 1

    day = sdate.day + days

    if day>calendar.monthrange(year,month)[1]:

        day -= calendar.monthrange(year,month)[1]

        month += 1

        if month>12:

            month = 1

            year += 1

    hour = sdate.hour + hours

    if hour>23:

        hour = 0

        day += 1

        if day>calendar.monthrange(year,month)[1]:

            day -= calendar.monthrange(year,month)[1]

            month += 1

            if month>12:

                month = 1

                year += 1

    return datetime(year,month,day,hour,sdate.minute)



# CHECK FOR NAN

def nan_check(x):

    if isinstance(x,float):

        if math.isnan(x):

            return True

    return False
df_test['ones'] = 1

dynamicPlot(df_test, 'ones', inc_dy=2, legend=0,

        title='Test Predictions. (Dotted line uses right y-axis. Solid uses left.)')