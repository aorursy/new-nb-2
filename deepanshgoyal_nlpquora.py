# This Python 3 environment comes with many helpful analytics libraries installed

# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python

# For example, here's several helpful packages to load in 



import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)



# Input data files are available in the "../input/" directory.

# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory



import os

from tqdm import tqdm



tqdm.pandas()



# Any results you write to the current directory are saved as output.
train= pd.read_csv("../input/train.csv")

test= pd.read_csv("../input/test.csv")

print('train shape:',train.shape)

print('test shape:',test.shape)
def build_vocab(sentences, verbose =  True):

    vocab = {}

    for sentence in tqdm(sentences, disable = (not verbose)):

        for word in sentence:

            try:

                vocab[word] += 1

            except KeyError:

                vocab[word] = 1

    return vocab
sentences = train["question_text"].progress_apply(lambda x: x.split()).values

vocab = build_vocab(sentences)

print({k: vocab[k] for k in list(vocab)[:5]})
train.info()
from gensim.models import KeyedVectors



news_path = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'

embeddings_index = KeyedVectors.load_word2vec_format(news_path, binary=True)
import operator 



def check_coverage(vocab,embeddings_index):

    a = {}

    oov = {}

    k = 0

    i = 0

    for word in tqdm(vocab):

        try:

            a[word] = embeddings_index[word]

            k += vocab[word]

        except:



            oov[word] = vocab[word]

            i += vocab[word]

            pass



    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))

    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))

    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]



    return sorted_x
oov = check_coverage(vocab,embeddings_index)
oov[:10]
'?' in embeddings_index
'&' in embeddings_index
def clean_text(x):



    x = str(x)

    for punct in "/-'":

        x = x.replace(punct, ' ')

    for punct in '&':

        x = x.replace(punct, f' {punct} ')

    for punct in '?!.,"#$%\'()*+-/:;<=>@[\\]^_`{|}~' + '“”’':

        x = x.replace(punct, '')

    return x

train["question_text"] = train["question_text"].progress_apply(lambda x: clean_text(x))

sentences = train["question_text"].apply(lambda x: x.split())

vocab = build_vocab(sentences)
oov = check_coverage(vocab,embeddings_index)
oov[:10]
for i in range(10):

    print(embeddings_index.index2entity[i])
import re



def clean_numbers(x):



    x = re.sub('[0-9]{5,}', '#####', x)

    x = re.sub('[0-9]{4}', '####', x)

    x = re.sub('[0-9]{3}', '###', x)

    x = re.sub('[0-9]{2}', '##', x)

    return x
train["question_text"] = train["question_text"].progress_apply(lambda x: clean_numbers(x))

sentences = train["question_text"].progress_apply(lambda x: x.split())

vocab = build_vocab(sentences)
oov = check_coverage(vocab,embeddings_index)
oov[:20]
def _get_mispell(mispell_dict):

    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))

    return mispell_dict, mispell_re





mispell_dict = {'colour':'color',

                'centre':'center',

                'didnt':'did not',

                'doesnt':'does not',

                'isnt':'is not',

                'shouldnt':'should not',

                'favourite':'favorite',

                'travelling':'traveling',

                'counselling':'counseling',

                'theatre':'theater',

                'cancelled':'canceled',

                'labour':'labor',

                'organisation':'organization',

                'wwii':'world war 2',

                'citicise':'criticize',

                'instagram': 'social medium',

                'whatsapp': 'social medium',

                'snapchat': 'social medium'



                }

mispellings, mispellings_re = _get_mispell(mispell_dict)



def replace_typical_misspell(text):

    def replace(match):

        return mispellings[match.group(0)]



    return mispellings_re.sub(replace, text)
train["question_text"] = train["question_text"].progress_apply(lambda x: replace_typical_misspell(x))

sentences = train["question_text"].progress_apply(lambda x: x.split())

to_remove = ['a','to','of','and']

sentences = [[word for word in sentence if not word in to_remove] for sentence in tqdm(sentences)]

vocab = build_vocab(sentences)
oov = check_coverage(vocab,embeddings_index)
oov[:20]