# Suppress warnings 

import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)

warnings.filterwarnings("ignore", category=UserWarning)

warnings.filterwarnings("ignore", category=FutureWarning)

from IPython.display import HTML





HTML('<iframe width="1106" height="622" src="https://www.youtube.com/embed/NZyQu1u3N9Y" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>')
import numpy as np # linear algebra

import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

import gc



# matplotlib and seaborn for plotting

import matplotlib.pyplot as plt




import seaborn as sns

import matplotlib.patches as patches



from plotly import tools, subplots

import plotly.offline as py

py.init_notebook_mode(connected=True)

import plotly.graph_objs as go

import plotly.express as px

pd.set_option('max_columns', 150)



py.init_notebook_mode(connected=True)

from plotly.offline import init_notebook_mode, iplot

init_notebook_mode(connected=True)

import plotly.graph_objs as go



import os,random, math, psutil, pickle    
print(os.listdir("../input/ashrae-energy-prediction/"))

root = '../input/ashrae-energy-prediction/'

train_df = pd.read_csv(root + 'train.csv')

train_df["timestamp"] = pd.to_datetime(train_df["timestamp"], format='%Y-%m-%d %H:%M:%S')



weather_train_df = pd.read_csv(root + 'weather_train.csv')

test_df = pd.read_csv(root + 'test.csv')

weather_test_df = pd.read_csv(root + 'weather_test.csv')

building_meta_df = pd.read_csv(root + 'building_metadata.csv')

sample_submission = pd.read_csv(root + 'sample_submission.csv')
print('Size of train_df data', train_df.shape)

print('Size of weather_train_df data', weather_train_df.shape)

print('Size of weather_test_df data', weather_test_df.shape)

print('Size of building_meta_df data', building_meta_df.shape)
## Function to reduce the DF size

def reduce_mem_usage(df, verbose=True):

    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']

    start_mem = df.memory_usage().sum() / 1024**2    

    for col in df.columns:

        col_type = df[col].dtypes

        if col_type in numerics:

            c_min = df[col].min()

            c_max = df[col].max()

            if str(col_type)[:3] == 'int':

                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:

                    df[col] = df[col].astype(np.int8)

                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:

                    df[col] = df[col].astype(np.int16)

                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:

                    df[col] = df[col].astype(np.int32)

                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:

                    df[col] = df[col].astype(np.int64)  

            else:

                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:

                    df[col] = df[col].astype(np.float16)

                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:

                    df[col] = df[col].astype(np.float32)

                else:

                    df[col] = df[col].astype(np.float64)    

    end_mem = df.memory_usage().sum() / 1024**2

    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))

    return df
train_df = reduce_mem_usage(train_df)

test_df = reduce_mem_usage(test_df)



weather_train_df = reduce_mem_usage(weather_train_df)

weather_test_df = reduce_mem_usage(weather_test_df)

building_meta_df = reduce_mem_usage(building_meta_df)
train_df.head()
train_df.columns.values
weather_train_df.head()
weather_train_df.columns.values
weather_test_df.head()
weather_test_df.columns.values
building_meta_df.head()
building_meta_df.columns.values
for key, d in train_df.groupby('meter_reading'):

    break

    d.head()

plt.figure(figsize = (20,5))

d['meter'].plot()
plt.figure(figsize=(8,6))

plt.scatter(range(train_df.shape[0]), np.sort(train_df['meter_reading'].values))

plt.xlabel('index', fontsize=12)

plt.ylabel('meter_reading', fontsize=12)

plt.title("Target Distribution", fontsize=14)

plt.show()
plt.figure(figsize = (15,5))

train_df['meter_reading'].plot()
train_df['meter_reading'].plot(kind='hist',

                            bins=25,

                            figsize=(15, 5),

                           title='Distribution of Target Variable (meter_reading)')

plt.show()
# Load data

train = train_df.set_index(['timestamp'])



# Plot missing values per building/meter

f,a=plt.subplots(1,4,figsize=(20,30))

for meter in np.arange(4):

    df = train[train.meter==meter].copy().reset_index()

    df['timestamp'] = pd.to_timedelta(df.timestamp).dt.total_seconds() / 3600

    df['timestamp'] = df.timestamp.astype(int)

    df.timestamp -= df.timestamp.min()

    missmap = np.empty((1449, df.timestamp.max()+1))

    missmap.fill(np.nan)

    for l in df.values:

        if l[2]!=meter:continue

        missmap[int(l[1]), int(l[0])] = 0 if l[3]==0 else 1

    a[meter].set_title(f'meter {meter:d}')

    sns.heatmap(missmap, cmap='Paired', ax=a[meter], cbar=False)
total = train_df.isnull().sum().sort_values(ascending = False)

percent = (train_df.isnull().sum()/train_df.isnull().count()*100).sort_values(ascending = False)

missing__train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

missing__train_data.head(4)
# checking missing data

total = weather_train_df.isnull().sum().sort_values(ascending = False)

percent = (weather_train_df.isnull().sum()/weather_train_df.isnull().count()*100).sort_values(ascending = False)

missing_weather_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

missing_weather_data.head(9)
# checking missing data

total = weather_test_df.isnull().sum().sort_values(ascending = False)

percent = (weather_test_df.isnull().sum()/weather_test_df.isnull().count()*100).sort_values(ascending = False)

missing_weather_test_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

missing_weather_test_data.head(9)
# checking missing data

total = building_meta_df.isnull().sum().sort_values(ascending = False)

percent = (building_meta_df.isnull().sum()/building_meta_df.isnull().count()*100).sort_values(ascending = False)

missing_building_meta_df  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])

missing_building_meta_df.head(6)
# Number of each type of column

train_df.dtypes.value_counts()
# Number of unique classes in each object column

train_df.select_dtypes('object').apply(pd.Series.nunique, axis = 0)
# Find correlations with the target and sort

correlations = train_df.corr()['meter_reading'].sort_values()



# Display correlations

print('Most Positive Correlations:\n', correlations.tail(15))

print('\nMost Negative Correlations:\n', correlations.head(15))
corrs = train_df.corr()

corrs
plt.figure(figsize = (20, 8))



# Heatmap of correlations

sns.heatmap(corrs, cmap = plt.cm.RdYlBu_r, vmin = -0.25, annot = True, vmax = 0.6)

plt.title('Correlation Heatmap');
train_df.building_id.nunique()
train_df['meter_reading'].hist(figsize=(16, 8))
def plot_dist_col(column):

    '''plot dist curves for train and test weather data for the given column name'''

    fig, ax = plt.subplots(figsize=(10, 10))

    sns.distplot(weather_train_df[column].dropna(), color='green', ax=ax).set_title(column, fontsize=16)

    sns.distplot(weather_test_df[column].dropna(), color='purple', ax=ax).set_title(column, fontsize=16)

    plt.xlabel(column, fontsize=15)

    plt.legend(['train', 'test'])

    plt.show()
plot_dist_col('air_temperature')
plot_dist_col('cloud_coverage')
plot_dist_col('dew_temperature')
plot_dist_col('precip_depth_1_hr')
plot_dist_col('sea_level_pressure')
plot_dist_col('wind_direction')
plot_dist_col('wind_speed')
from statsmodels.tsa.seasonal import seasonal_decompose
ts=train_df.groupby(["timestamp"])["meter_reading"].sum()

ts.astype('float')

plt.figure(figsize=(16,8))

plt.title('meter_reading')

plt.xlabel('timestamp')

plt.ylabel('meter_reading')

plt.plot(ts);
plt.figure(figsize=(16,6))

plt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');

plt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');

plt.legend();
import statsmodels.api as sm

# multiplicative

res = sm.tsa.seasonal_decompose(ts.values,freq=12,model="multiplicative")

fig = res.plot()
# Additive model

res = sm.tsa.seasonal_decompose(ts.values,freq=12,model="additive")

fig = res.plot()
y_mean_time = train_df.groupby('timestamp').meter_reading.mean()

y_mean_time.plot(figsize=(20, 8))
y_mean_time.rolling(window=10).std().plot(figsize=(20, 8))

ax = plt.axhline(y=0.009, color='red')
y_mean_time.rolling(window=10).std().plot(figsize=(20, 8))

plt.axhline(y=0.009, color='red')

plt.axvspan(0, 905, color='green', alpha=0.1)

plt.axvspan(906, 1505, color='red', alpha=0.1)
train_df['meter'] = pd.Categorical(train_df['meter']).rename_categories({0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'})

daily_train = train_df.copy()

daily_train['date'] = daily_train['timestamp'].dt.date

daily_train = daily_train.groupby(['date', 'building_id', 'meter']).sum()

daily_train
daily_train_agg = daily_train.groupby(['date', 'meter']).agg(['sum', 'mean', 'idxmax', 'max'])

daily_train_agg = daily_train_agg.reset_index()

level_0 = daily_train_agg.columns.droplevel(0)

level_1 = daily_train_agg.columns.droplevel(1)

level_0 = ['' if x == '' else '-' + x for x in level_0]

daily_train_agg.columns = level_1 + level_0

daily_train_agg.rename_axis(None, axis=1)

daily_train_agg.head()
fig_total = px.line(daily_train_agg, x='date', y='meter_reading-sum', color='meter', render_mode='svg')

fig_total.update_layout(title='Total kWh per energy aspect')

fig_total.show()
fig_maximum = px.line(daily_train_agg, x='date', y='meter_reading-max', color='meter', render_mode='svg')

fig_maximum.update_layout(title='Maximum kWh value per energy aspect')

fig_maximum.show()
daily_train_agg['building_id_max'] = [x[1] for x in daily_train_agg['meter_reading-idxmax']]

daily_train_agg.head()
def show_building(building, energy_aspects=None):

    fig = px.line(daily_train.loc[(slice(None), building, slice(None)), :].reset_index(),

                  x='date',

                  y='meter_reading',

                  color='meter',

                  render_mode='svg')

    if energy_aspects:

        if 'electricity' not in energy_aspects:

            fig['data'][0].visible = 'legendonly'

        if 'chilledwater' not in energy_aspects:

            fig['data'][1].visible = 'legendonly'

        if 'steam' not in energy_aspects:

            fig['data'][2].visible = 'legendonly'

        if 'hotwater' not in energy_aspects:

            fig['data'][3].visible = 'legendonly'

    fig.update_layout(title='Building ID: {}'.format(building))        

    fig.show()

    display(building_metadata[building_metadata['building_id']==building])
print('Number of days that a building has the maximum electricity consumption of all the buildings:\n')

print(daily_train_agg[daily_train_agg['meter'] == 'electricity']['building_id_max'].value_counts())
daily_train_electricity = daily_train_agg[daily_train_agg['meter']=='electricity'].copy()

daily_train_electricity['building_id_max'] = pd.Categorical(daily_train_electricity['building_id_max'])

fig_daily_electricity = px.scatter(daily_train_electricity,

                                   x='date',

                                   y='meter_reading-max',

                                   color='building_id_max',

                                   render_mode='svg')

fig_daily_electricity.update_layout(title='Maximum consumption values for the day and energy aspect')

fig_daily_electricity.show()
print('Number of days that a building has the maximum chilledwater consumption of all the buildings:\n')

print(daily_train_agg[daily_train_agg['meter'] == 'chilledwater']['building_id_max'].value_counts())
daily_train_chilledwater = daily_train_agg[daily_train_agg['meter']=='chilledwater'].copy()

daily_train_chilledwater['building_id_max'] = pd.Categorical(daily_train_chilledwater['building_id_max'])

fig_daily_chilledwater = px.scatter(daily_train_chilledwater,

                                    x='date',

                                    y='meter_reading-max',  

                                    color='building_id_max', 

                                    render_mode='svg')

fig_daily_chilledwater.update_layout(title='Maximum consumption values for the day and energy aspect')

fig_daily_chilledwater.show()
print('Number of days that a building has the maximum steam consumption of all the buildings:\n')

print(daily_train_agg[daily_train_agg['meter'] == 'steam']['building_id_max'].value_counts())
daily_train_steam = daily_train_agg[daily_train_agg['meter']=='steam'].copy()

daily_train_steam['building_id_max'] = pd.Categorical(daily_train_steam['building_id_max'])

fig_daily_steam = px.scatter(daily_train_steam,

                             x='date',

                             y='meter_reading-max',

                             color='building_id_max',

                             render_mode='svg')

fig_daily_steam.update_layout(title='Maximum consumption values for the day and energy aspect')

fig_daily_steam.show()
print('Number of days that a building has the maximum hotwater consumption of all the buildings:\n')

print(daily_train_agg[daily_train_agg['meter'] == 'hotwater']['building_id_max'].value_counts())
daily_train_hotwater = daily_train_agg[daily_train_agg['meter']=='hotwater'].copy()

daily_train_hotwater['building_id_max'] = pd.Categorical(daily_train_hotwater['building_id_max'])

fig_daily_hotwater = px.scatter(daily_train_hotwater,

                                x='date',

                                y='meter_reading-max',

                                color='building_id_max',

                                render_mode='svg')

fig_daily_hotwater.update_layout(title='Maximum consumption values for the day and energy aspect')

fig_daily_hotwater.show()
from sklearn.preprocessing import LabelEncoder

from sklearn import metrics
from sklearn.metrics import mean_squared_error

import lightgbm as lgb

from sklearn.model_selection import train_test_split
train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])

test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])

weather_train_df['timestamp'] = pd.to_datetime(weather_train_df['timestamp'])

weather_test_df['timestamp'] = pd.to_datetime(weather_test_df['timestamp'])

    

building_meta_df['primary_use'] = building_meta_df['primary_use'].astype('category')
temp_df = train_df[['building_id']]

temp_df = temp_df.merge(building_meta_df, on=['building_id'], how='left')

del temp_df['building_id']

train_df = pd.concat([train_df, temp_df], axis=1)



temp_df = test_df[['building_id']]

temp_df = temp_df.merge(building_meta_df, on=['building_id'], how='left')



del temp_df['building_id']

test_df = pd.concat([test_df, temp_df], axis=1)

del temp_df, building_meta_df

temp_df = train_df[['site_id','timestamp']]

temp_df = temp_df.merge(weather_train_df, on=['site_id','timestamp'], how='left')



del temp_df['site_id'], temp_df['timestamp']

train_df = pd.concat([train_df, temp_df], axis=1)



temp_df = test_df[['site_id','timestamp']]

temp_df = temp_df.merge(weather_test_df, on=['site_id','timestamp'], how='left')



del temp_df['site_id'], temp_df['timestamp']

test_df = pd.concat([test_df, temp_df], axis=1)



del temp_df, weather_train_df, weather_test_df
train_df.to_pickle('train_df.pkl')

test_df.to_pickle('test_df.pkl')

   

del train_df, test_df

gc.collect()
train_df = pd.read_pickle('train_df.pkl')

test_df = pd.read_pickle('test_df.pkl')
train_df['age'] = train_df['year_built'].max() - train_df['year_built'] + 1

test_df['age'] = test_df['year_built'].max() - test_df['year_built'] + 1
le = LabelEncoder()

# train_df['primary_use'] = train_df['primary_use'].astype(str)

train_df['primary_use'] = le.fit_transform(train_df['primary_use']).astype(np.int8)



# test_df['primary_use'] = test_df['primary_use'].astype(str)

test_df['primary_use'] = le.fit_transform(test_df['primary_use']).astype(np.int8)



train_df['floor_count'] = train_df['floor_count'].fillna(-999).astype(np.int16)

test_df['floor_count'] = test_df['floor_count'].fillna(-999).astype(np.int16)



train_df['year_built'] = train_df['year_built'].fillna(-999).astype(np.int16)

test_df['year_built'] = test_df['year_built'].fillna(-999).astype(np.int16)



train_df['age'] = train_df['age'].fillna(-999).astype(np.int16)

test_df['age'] = test_df['age'].fillna(-999).astype(np.int16)



train_df['cloud_coverage'] = train_df['cloud_coverage'].fillna(-999).astype(np.int16)

test_df['cloud_coverage'] = test_df['cloud_coverage'].fillna(-999).astype(np.int16) 

train_df['month_datetime'] = train_df['timestamp'].dt.month.astype(np.int8)

train_df['weekofyear_datetime'] = train_df['timestamp'].dt.weekofyear.astype(np.int8)

train_df['dayofyear_datetime'] = train_df['timestamp'].dt.dayofyear.astype(np.int16)

    

train_df['hour_datetime'] = train_df['timestamp'].dt.hour.astype(np.int8)  

train_df['day_week'] = train_df['timestamp'].dt.dayofweek.astype(np.int8)

train_df['day_month_datetime'] = train_df['timestamp'].dt.day.astype(np.int8)

train_df['week_month_datetime'] = train_df['timestamp'].dt.day/7

train_df['week_month_datetime'] = train_df['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)

    

train_df['year_built'] = train_df['year_built']-1900

train_df['square_feet'] = np.log(train_df['square_feet'])

    

test_df['month_datetime'] = test_df['timestamp'].dt.month.astype(np.int8)

test_df['weekofyear_datetime'] = test_df['timestamp'].dt.weekofyear.astype(np.int8)

test_df['dayofyear_datetime'] = test_df['timestamp'].dt.dayofyear.astype(np.int16)

    

test_df['hour_datetime'] = test_df['timestamp'].dt.hour.astype(np.int8)

test_df['day_week'] = test_df['timestamp'].dt.dayofweek.astype(np.int8)

test_df['day_month_datetime'] = test_df['timestamp'].dt.day.astype(np.int8)

test_df['week_month_datetime'] = test_df['timestamp'].dt.day/7

test_df['week_month_datetime'] = test_df['week_month_datetime'].apply(lambda x: math.ceil(x)).astype(np.int8)

    

test_df['year_built'] = test_df['year_built']-1900

test_df['square_feet'] = np.log(test_df['square_feet'])
HTML('<iframe width="829" height="622" src="https://www.youtube.com/embed/ABAR8TIwce4" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>')
HTML('<iframe width="847" height="622" src="https://www.youtube.com/embed/wjRJsbj3X00" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>')