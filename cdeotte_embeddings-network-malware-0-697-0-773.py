# SET THIS VARIABLE TO TRUE TO RUN KERNEL QUICKLY

# AND TEST FOR BUGS. ONLY 10000 ROWS OF DATA IS LOADED

Debug = False



# IMPORT LIBRARIES

import pandas as pd, numpy as np, os, gc



# LOAD THESE BUT DONT ENCODE

MM = ['AvSigVersion','Census_OSVersion','Census_OSBuildRevision','AppVersion','EngineVersion']



# LOAD AND NUMERIC ENCODE

NE = ['Census_SystemVolumeTotalCapacity','Census_PrimaryDiskTotalCapacity']



# LOAD AND STATISTICAL ONE-HOT-ENCODE

OHE = [ 'RtpStateBitfield','DefaultBrowsersIdentifier', 'AVProductStatesIdentifier',

        'AVProductsInstalled', 'AVProductsEnabled', 'CountryIdentifier', 'CityIdentifier', 

        'GeoNameIdentifier', 'LocaleEnglishNameIdentifier', 'Processor', 'OsBuild', 'OsSuite',

        'SmartScreen','Census_MDC2FormFactor', 'Census_OEMNameIdentifier', 

        'Census_ProcessorCoreCount', 'Census_ProcessorModelIdentifier', 

        'Census_OSUILocaleIdentifier', 'Census_PrimaryDiskTypeName',

        'Census_HasOpticalDiskDrive', 'Census_TotalPhysicalRAM', 'Census_ChassisTypeName',

        'Census_InternalPrimaryDiagonalDisplaySizeInInches',

        'Census_InternalPrimaryDisplayResolutionHorizontal',

        'Census_InternalPrimaryDisplayResolutionVertical',

        'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',

        'Census_InternalBatteryNumberOfCharges', 'Census_OSEdition', 'Census_GenuineStateName',

        'Census_ActivationChannel', 'Census_FirmwareManufacturerIdentifier', 'Census_IsTouchEnabled', 

        'Census_IsPenCapable', 'Census_IsAlwaysOnAlwaysConnectedCapable', 'Wdft_IsGamer', 

        'Wdft_RegionIdentifier', 'OsBuildLab', 'OrganizationIdentifier','Platform',

        'Census_OEMModelIdentifier', 'IsProtected', 'IeVerIdentifier','Firewall', 

        'Census_ProcessorManufacturerIdentifier','Census_OSInstallTypeName',

        'Census_OSWUAutoUpdateOptionsName','Census_IsFlightingInternal',

        'Census_FlightRing','Census_ThresholdOptIn','Census_FirmwareVersionIdentifier',

        'Census_IsSecureBootEnabled','Census_IsWIMBootEnabled']



# DONT LOAD THESE

XX = ['SMode','IsBeta', 'OsVer', 'OsPlatformSubRelease', 'SkuEdition', 'AutoSampleOptIn', 'PuaMode',

     'UacLuaenable', 'Census_ProcessorClass', 'Census_OSArchitecture', 'Census_OSBranch',

     'Census_OSBuildNumber', 'Census_OSSkuName', 'Census_OSInstallLanguageIdentifier',

     'Census_IsPortableOperatingSystem', 'Census_IsFlightsDisabled', 'Census_IsVirtualDevice',

     'IsSxsPassiveMode','ProductName','HasTpm','Census_DeviceFamily']



# DONT LOAD THIS

XXX = ['MachineIdentifier']



# LOAD ALL AS CATEGORIES

dtypes = {}

for x in OHE+NE+MM: dtypes[x] = 'category'

dtypes['HasDetections'] = 'int8'



# LOAD TRAIN CSV FILE

if Debug:

    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv', usecols=dtypes.keys(), dtype=dtypes,nrows=10000)

else:

    df_train = pd.read_csv('../input/microsoft-malware-prediction/train.csv', usecols=dtypes.keys(), dtype=dtypes)

if 5244810 in df_train.index:

    df_train.loc[5244810,'AvSigVersion'] = '1.273.1144.0'

    df_train['AvSigVersion'].cat.remove_categories('1.2&#x17;3.1144.0',inplace=True)

print ('Loaded',len(df_train),'rows of TRAIN.CSV!')



# SHUFFLE TRAIN DATA

df_train = df_train.sample(frac=1)

df_train.reset_index(drop=True,inplace=True)



# LOAD TEST CSV FILE

if Debug:

    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes,nrows=10000)

else:

    df_test = pd.read_csv('../input/microsoft-malware-prediction/test.csv', usecols=list(dtypes.keys())[0:-1], dtype=dtypes)

print ('Loaded',len(df_test),'rows of TEST.CSV!')
import math



# FACTORIZE

def factor_data(df_train, df_test, col):

    df_comb = pd.concat([df_train[col],df_test[col]],axis=0)

    df_comb,_ = df_comb.factorize(sort=True)

    # MAKE SMALLEST LABEL 1, RESERVE 0

    df_comb += 1

    # MAKE NAN LARGEST LABEL

    df_comb = np.where(df_comb==0, df_comb.max()+1, df_comb)

    df_train[col] = df_comb[:len(df_train)]

    df_test[col] = df_comb[len(df_train):]

    del df_comb

    mx = max(df_train[col].max(),df_test[col].max())+1

    return mx

    

# OPTIMIZE MEMORY

def reduce_memory(df,col):

    mx = df[col].max()

    if mx<256:

            df[col] = df[col].astype('uint8')

    elif mx<65536:

        df[col] = df[col].astype('uint16')

    else:

        df[col] = df[col].astype('uint32')

    

# LOG FREQUENCY ENCODE

def encode_FE_lg(df,col,verbose=1):

    ln = 1/df[col].nunique()

    vc = (df[col].value_counts(dropna=False, normalize=True)+ln).map(math.log).to_dict()

    nm = col+'_FE_lg'

    df[nm] = df[col].map(vc)

    df[nm] -= df[nm].min()

    df[nm] = df[nm]/df[nm].max()

    df[nm] = df[nm].astype('float32')

    if verbose==1:

        print('FE encoded',col)

    return [nm]



# STATISTICAL CATEGORY ENCODE

def encode_CE(df, col, filter, zscore, tar='HasDetections', m=0.5, verbose=1):

    cv = pd.DataFrame( df[col].value_counts(dropna=False) ).reset_index()

    cv4 = df.groupby(col)[tar].mean().reset_index().rename({tar:'rate',col:'index'},axis=1)

    d1 = set(cv['index'].unique())

    cv = pd.merge(cv,cv4,on='index',how='left')

    if (len( cv[ cv['index'].isna() ])!=0 ):

        cv.loc[ cv['index'].isna(),'rate' ] = df.loc[ df[col].isna(),tar ].mean()

    cv = cv[ cv[col]> (filter * len(df)) ]

    cv['ratec'] = (df[tar].sum() - cv['rate']*cv[col])/(len(df)-cv[col])

    cv['sd'] = zscore * 0.5 / cv[col].map(lambda x: math.sqrt(x))

    cv = cv[ (abs(cv['rate']-m)>=cv['sd']) | (abs(cv['ratec']-1+m)>=cv['sd']) ]

    d2 = set(cv['index'].unique())

    d = list(d1 - d2)

    if (df[col].dtype.name=='category'):

        if (not 0 in df[col].cat.categories):

            df[col].cat.add_categories(0,inplace=True)

        else:

            print('###WARNING CAT 0 ALREADY EXISTS IN',col)

    df.loc[ df[col].isin(d),col ] = 0

    if verbose==1:

        print('CE encoded',col,'-',len(d2),'values. Removed',len(d),'values')

    mx = df[col].nunique()

    return [mx,d2]



# CATEGORY ENCODE FROM KEEP LIST

def encode_CE_test(df,col,d):

    if (df[col].dtype.name=='category'):

        if (not 0 in df[col].cat.categories):

            df[col].cat.add_categories(0,inplace=True)

        else:

            print('###WARNING CAT 0 ALREADY EXISTS IN',col)

    df.loc[ ~df[col].isin(d),col ] = 0

    mx = df[col].nunique()

    return [mx,d]
def makeNew(df,verbose=1,add=0,TS=True,data=0):



    old = df.columns

    

    # FEATURE ENGINEER

    df['AppVersion2'] = df['AppVersion'].apply(lambda x: x.split('.')[1]).astype('category')



    if TS:

        from datetime import datetime, date, timedelta



        # AS timestamp

        datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]

        df['DateAS'] = df['AvSigVersion'].map(datedictAS)



        # OS timestamp

        datedictOS = np.load('../input/malware-timestamps-2/OSVersionTimestamps.npy')[()]

        df['DateOS'] = df['Census_OSVersion'].map(datedictOS)



        df['Lag1'] = df['DateAS'] - df['DateOS']

        df['Lag1'] = df['Lag1'].map(lambda x: x.days//7)

        df['Lag1'] = df['Lag1']/52.0

        df['Lag1'] = df['Lag1'].astype('float32')

        df['Lag1'].fillna(0,inplace=True)

        

        if data!=0:

            if data==1:

                df['Lag5'] = datetime(2018,7,26) - df['DateAS'] # TRAIN

            elif data==2:

                df['Lag5'] = datetime(2018,9,27) - df['DateAS'] #PUBLIC TEST

            elif data==3:

                df['Lag5'] = datetime(2018,10,27) - df['DateAS'] #PRIVATE TEST

            df['Lag5'] = df['Lag5'].map(lambda x: x.days//1)

            df.loc[ df['Lag5']<0, 'Lag5' ] = 0

            df['Lag5'] = df['Lag5']/365.0

            df['Lag5'] = df['Lag5'].astype('float32')

            df['Lag5'].fillna(0,inplace=True)



        del df['DateAS'], df['DateOS']

        del datedictAS, datedictOS

        x=gc.collect()

    

    # NUMERIC ENCODE NE VARIABLES

    for col in NE:

        nm = col+'_NE'

        df[nm] = df[col].astype('float32')

        df[nm] /= np.std(df[nm])

    new = list(set(df.columns)-set(old))

    ret = []

    for x in new:

        if str(df[x].dtype)=='category': # if cat

            if add==1: OHE.append(x)

        else: 

            ret.append(x)

            df[x].fillna(df[x].mean(),inplace=True)

    if verbose==1:

        print('Engineered',len(new),'new features!')

    return ret
# GET FREQUENCY ENCODE LIST

FE = []

for col in df_train.columns:

    if col=='HasDetections': continue

    if df_train[col].nunique()>10:

        FE.append(col)



# FEATURE ENGINEER / NEUMERIC ENCODE

NUM = makeNew(df_train,verbose=0,add=1,data=1)

makeNew(df_test,verbose=0,data=2)

print('Engineered '+str(len(NUM))+' variables (including NE)')

ct = len(NUM)+1; cnew = ct

    

# FREQUENCY ENCODE

for x in FE:

    NUM += encode_FE_lg(df_train,x,verbose=0)

    encode_FE_lg(df_test,x,verbose=0)

    #print(str(ct)+': FE: '+x)

    ct += 1

print('Frequency encoded '+str(len(NUM)-cnew)+' variables')

    

# STATISTICAL CATEGORY ENCODE

inps={}; tt = 0

for col in OHE:

    factor_data(df_train,df_test,col)

    d = encode_CE(df_train,col,0.001,1)[1]

    encode_CE_test(df_test,col,d)

    inps[col] = factor_data(df_train,df_test,col)

    tt += inps[col]

    reduce_memory(df_train,col)

    reduce_memory(df_test,col)

    #print(str(ct)+': CE: '+col)

    ct += 1



# REMOVE UNNEEDED

for x in np.unique(NE+MM):

    del df_train[x]

    if x!='AvSigVersion': del df_test[x]

x = gc.collect()



mm = round(df_train.memory_usage(deep=True).sum() / 1024**2)

mm2 = round(df_test.memory_usage(deep=True).sum() / 1024**2)

print('Encoded '+str(len(NUM))+' non-CE variables and '+str(len(OHE))+' CE containing '+str(tt)+' unique values into '+str(mm)+' Mb memory')

print('Test memory is '+str(mm2)+' Mb')
from keras import callbacks

from sklearn.metrics import roc_auc_score



class printAUC(callbacks.Callback):

    def __init__(self, X_train, y_train, inps, fes, X_val, y_val, k, ee):

        super(printAUC, self).__init__()

        self.bestAUC = 0

        self.X_train = X_train

        self.y_train = y_train

        self.inps = inps

        self.fes = fes

        self.X_val = X_val

        self.y_val = y_val

        self.k = k

        self.ee = ee

        

    def on_epoch_end(self, epoch, logs={}):

        pred = self.model.predict([self.X_train[col] for col in self.inps] + [self.X_train[self.fes]])

        aucTR = roc_auc_score(self.y_train, pred)

        pred = self.model.predict([self.X_val[col] for col in self.inps] + [self.X_val[self.fes]])

        auc = roc_auc_score(self.y_val, pred)

        print ("Train AUC: " + str(round(aucTR,5))+" - Validation AUC: " + str(round(auc,5)))

        if (self.bestAUC < auc) :

            self.bestAUC = auc

            self.model.save("bestNet"+str(self.k)+".h5", overwrite=True)

        return
# DEFINE NETWORK ARCHITECTURE GROUPINGS

# (1) GEOGRAPHICAL, (2) SOFTWARE/VIRUS, (3) HARDWARE, (4) NAME/MODEL

groups = [  ['CountryIdentifier','CityIdentifier','OrganizationIdentifier','GeoNameIdentifier',

             'LocaleEnglishNameIdentifier','Census_OSInstallLanguageIdentifier','Census_OSUILocaleIdentifier',

            'Wdft_RegionIdentifier'],

            ['DefaultBrowsersIdentifier', 'AVProductStatesIdentifier', 'AVProductsInstalled', 'AVProductsEnabled',

             'IsProtected', 'SMode', 'IeVerIdentifier', 'SmartScreen', 'Firewall','Census_IsSecureBootEnabled',

            'Census_IsWIMBootEnabled','Wdft_IsGamer','Census_OSWUAutoUpdateOptionsName','Census_GenuineStateName',

            'AppVersion2'],

            ['Processor','Census_MDC2FormFactor','Census_DeviceFamily','Census_ProcessorCoreCount','Census_ProcessorClass',

            'Census_PrimaryDiskTypeName','Census_HasOpticalDiskDrive','Census_TotalPhysicalRAM','Census_ChassisTypeName',

            'Census_InternalPrimaryDiagonalDisplaySizeInInches', 'Census_InternalPrimaryDisplayResolutionHorizontal',

            'Census_InternalPrimaryDisplayResolutionVertical', 'Census_PowerPlatformRoleName', 'Census_InternalBatteryType',

            'Census_InternalBatteryNumberOfCharges','Census_IsTouchEnabled','Census_IsPenCapable',

             'Census_IsAlwaysOnAlwaysConnectedCapable'],

            ['Census_OEMNameIdentifier', 'Census_OEMModelIdentifier', 'Census_ProcessorManufacturerIdentifier',

            'Census_ProcessorModelIdentifier','Census_FirmwareManufacturerIdentifier', 'Census_FirmwareVersionIdentifier']

         ]
from keras.models import Model

from keras.layers import Dense, Input, concatenate, BatchNormalization, Activation, Dropout, Embedding, Reshape

from keras.callbacks import LearningRateScheduler

from keras.optimizers import Adam



df_train_Y = df_train['HasDetections']

del df_train['HasDetections']

x=gc.collect()



#SPLIT TRAIN AND VALIDATION SET

chunk = len(df_train)//5

idx = range(chunk*0,chunk//2)

idx2 = range(chunk//2,chunk)

idx3 = range(chunk,chunk*3)

idx4 = range(chunk*3,chunk*5)

X_val1 = df_train.loc[idx]

Y_val1 = df_train_Y.loc[idx]

X_val2 = df_train.loc[idx2]

Y_val2 = df_train_Y.loc[idx2]

X_train1 = df_train.loc[idx3]

Y_train1 = df_train_Y.loc[idx3]

X_train2 = df_train.loc[idx4]

Y_train2 = df_train_Y.loc[idx4]

del df_train, df_train_Y

x=gc.collect()
ins = []; outs = {}

# CREATE AN EMBEDDING FOR EACH CATEGORY VARIABLE

for k in inps.keys():

    x = Input(shape=(1,))

    ins.append(x)

    y = np.int(inps[k])

    x = Embedding(y, y, input_length=1)(x)

    x = Reshape(target_shape=(y, ))(x)

    outs[k]=x 

    

# ORGANIZE EMBEDDINGS INTO GROUPS

all = set(inps.keys())

used = []

outs2 = []

for k in groups:

    g = [outs[x] for x in set(k).intersection(all)]

    used += list(set(k).intersection(all))

    x = concatenate(g)

    s = sum([inps[x] for x in set(k).intersection(all)])

    x = Dense(s//2,kernel_initializer='he_uniform')(x)

    x = BatchNormalization()(x)

    x = Activation('elu')(x)

    outs2.append(x)

g = [outs[x] for x in all-set(used)]

x = concatenate(g)

s = sum([inps[x] for x in all-set(used)])

x = Dense(s//2,kernel_initializer='he_uniform')(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

outs2.append(x)



# ORGANIZE FREQUENCY ENCODED AND NUMERICS INTO A GROUP

x = Input(shape=(len(NUM), ))

ins.append(x)

x = Dense(len(NUM)//2,kernel_initializer='he_uniform')(x)

x = BatchNormalization()(x)

x = Activation('elu')(x) 



# CONNECT GROUPS TO DENSE LAYERS

x = concatenate(outs2+[x])

x = Dense(100,kernel_initializer='he_uniform')(x)

x = Dropout(0.2)(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

x = Dense(100,kernel_initializer='he_uniform')(x)

x = Dropout(0.2)(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

x = Dense(100,kernel_initializer='he_uniform')(x)

x = Dropout(0.2)(x)

x = BatchNormalization()(x)

x = Activation('elu')(x)

x = Dense(1,activation='sigmoid')(x)



model = Model(inputs=ins, outputs=x)

model.compile(optimizer=Adam(lr=0.01), loss='binary_crossentropy', metrics=['accuracy'])

#annealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)
epochs=10

batch=256

for k in range(epochs):

    # SPLIT TRAINING DATA IN HALF TO FIT INTO GPU MEMORY

    model.fit( [X_train1[col] for col in OHE] + [X_train1[NUM]],Y_train1,

        batch_size=batch, epochs = 1, verbose=2, callbacks=[ #annealer, 

        printAUC(X_train1, Y_train1, OHE, NUM, X_val1, Y_val1, 0, k)],

        validation_data = ([X_val1[col] for col in OHE] + [X_val1[NUM]],Y_val1) )

    model.fit( [X_train2[col] for col in OHE] + [X_train2[NUM]],Y_train2,

        batch_size=batch, epochs = 1, verbose=2, callbacks=[ #annealer, 

        printAUC(X_train2, Y_train2, OHE, NUM, X_val2, Y_val2, 0, k)],

        validation_data = ([X_val2[col] for col in OHE] + [X_val2[NUM]],Y_val2) )

    # SHUFFLE TRAIN

    X_train1['HasDetections'] = Y_train1

    X_train1 = X_train1.sample(frac=1)

    Y_train1 = X_train1['HasDetections']

    del X_train1['HasDetections']

    X_train2['HasDetections'] = Y_train2

    X_train2 = X_train2.sample(frac=1)

    Y_train2 = X_train2['HasDetections']

    del X_train2['HasDetections']

    x=gc.collect()
del model

del X_train1, Y_train1, X_val1, Y_val1

del X_train2, Y_train2, X_val2, Y_val2

del ins, outs, outs2, x

x = gc.collect()
# LOAD BEST SAVED NET

from keras.models import load_model



# PREDICT TEST

pred = np.zeros((len(df_test),1))

print('Predicting test...')

model = load_model('bestNet0.h5')

idx = 0; chunk = 1000000

if Debug: chunk = 5000

ct2 = 1;

while idx < len(df_test):

    idx2 = min(idx + chunk, len(df_test) )

    idx = range(idx, idx2)

    pred[idx] += model.predict( [df_test.iloc[idx][col] for col in OHE] + [df_test.iloc[idx][NUM]] )

    print(' part '+str(ct2)+' done')

    ct2 += 1

    idx = idx2

del model

x = gc.collect()
from datetime import datetime

datedictAS = np.load('../input/malware-timestamps/AvSigVersionTimestamps.npy')[()]

df_test['Date'] = df_test['AvSigVersion'].map(datedictAS)

df_test['HasDetections'] = pred

df_test['X'] = df_test['Date'] - datetime(2018,11,20,4,0) 

df_test['X'] = df_test['X'].map(lambda x: x.total_seconds()/86400)

df_test['X'].fillna(0,inplace=True)

s = 5.813888

df_test['F'] = 1.0

df_test['F'] = 1 - df_test['X']/s

df_test.loc[df_test['X']<=0,'F'] = 1.0

df_test.loc[df_test['X']>s,'F'] = 0

df_test['HasDetections'] *= df_test['F']

pred = df_test['HasDetections']
print('Writing submission file...')

if Debug:

    submit = pd.read_csv('../input/microsoft-malware-prediction/sample_submission.csv', nrows=10000)

else:

    submit = pd.read_csv('../input/microsoft-malware-prediction/sample_submission.csv')

submit['HasDetections'] = pred

submit.to_csv('submission.csv', index=False)

print('Done!')
import matplotlib.pyplot as plt    

b = plt.hist(pred, bins=200)
import calendar, math



def dynamicPlot(data,col, target='HasDetections', start=datetime(2018,4,1), end=datetime(2018,12,1)

                ,inc_hr=0,inc_dy=7,inc_mn=0,show=0.99,top=5,top2=4,title='',legend=1,z=0,dots=False):

    # check for timestamps

    if 'Date' not in data:

        print('Error dynamicPlot: DataFrame needs column Date of datetimes')

        return

    

    # remove detection line if category density is too small

    cv = data[(data['Date']>start) & (data['Date']<end)][col].value_counts(dropna=False)

    cvd = cv.to_dict()

    nm = cv.index.values

    th = show * len(data)

    sum = 0; lnn2 = 0

    for x in nm:

        lnn2 += 1

        sum += cvd[x]

        if sum>th:

            break

    top = min(top,len(nm))

    top2 = min(top2,len(nm),lnn2,top)



    # calculate rate within each time interval

    diff = (end-start).days*24*3600 + (end-start).seconds

    size = diff//(3600*((inc_mn * 28 + inc_dy) * 24 + inc_hr)) + 5

    data_counts = np.zeros([size,2*top+1],dtype=float)

    idx=0; idx2 = {}

    for i in range(top):

        idx2[nm[i]] = i+1

    low = start

    high = add_time(start,inc_mn,inc_dy,inc_hr)

    data_times = [low+(high-low)/2]

    while low<end:

        slice = data[ (data['Date']<high) & (data['Date']>=low) ]

        #data_counts[idx,0] = len(slice)

        data_counts[idx,0] = 5000*len(slice['AvSigVersion'].unique())

        for key in idx2:

            if nan_check(key): slice2 = slice[slice[col].isna()]

            else: slice2 = slice[slice[col]==key]

            data_counts[idx,idx2[key]] = len(slice2)

            if target in data:

                data_counts[idx,top+idx2[key]] = slice2['HasDetections'].mean()

        low = high

        high = add_time(high,inc_mn,inc_dy,inc_hr)

        data_times.append(low+(high-low)/2)

        idx += 1



    # plot lines

    fig = plt.figure(1,figsize=(15,3))

    cl = ['r','g','b','y','m']

    ax3 = fig.add_subplot(1,1,1)

    lines = []; labels = []

    if z==1: ax3.plot(data_times,data_counts[0:idx+1,0],'k')

    for i in range(top):

        tmp, = ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5])

        if dots: ax3.plot(data_times,data_counts[0:idx+1,i+1],cl[i%5]+'o')

        lines.append(tmp)

        labels.append(str(nm[i]))

    ax3.spines['left'].set_color('red')

    ax3.yaxis.label.set_color('red')

    ax3.tick_params(axis='y', colors='red')

    if col!='ones': ax3.set_ylabel('Category Density', color='r')

    else: ax3.set_ylabel('Data Density', color='r')

    #ax3.set_yticklabels([])

    if target in data:

        ax4 = ax3.twinx()

        for i in range(top2):

            ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+":")

            if dots: ax4.plot(data_times,data_counts[0:idx+1,i+1+top],cl[i%5]+"o")

        ax4.spines['left'].set_color('red')

        ax4.set_ylabel('Detection Rate', color='k')

    if title!='': plt.title(title)

    if legend==1: plt.legend(lines,labels,loc=2)

    plt.show()

        

# INCREMENT A DATETIME

def add_time(sdate,months=0,days=0,hours=0):

    month = sdate.month -1 + months

    year = sdate.year + month // 12

    month = month % 12 + 1

    day = sdate.day + days

    if day>calendar.monthrange(year,month)[1]:

        day -= calendar.monthrange(year,month)[1]

        month += 1

        if month>12:

            month = 1

            year += 1

    hour = sdate.hour + hours

    if hour>23:

        hour = 0

        day += 1

        if day>calendar.monthrange(year,month)[1]:

            day -= calendar.monthrange(year,month)[1]

            month += 1

            if month>12:

                month = 1

                year += 1

    return datetime(year,month,day,hour,sdate.minute)



# CHECK FOR NAN

def nan_check(x):

    if isinstance(x,float):

        if math.isnan(x):

            return True

    return False
df_test['ones'] = 1

dynamicPlot(df_test, 'ones', inc_dy=2, legend=0,

        title='Test.csv HasDetections Predictions. (Dotted line uses right y-axis. Solid uses left.)')